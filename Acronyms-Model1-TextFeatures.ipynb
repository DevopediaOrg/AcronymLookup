{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine-Learning Approach for Cross-Domain Acronym Definition Identification\n",
    "### Maya Varma and Rachel Gardner\n",
    "### Autumn 2017\n",
    "\n",
    "The large quantity of data uploaded to the Internet daily presents numerous challenges in information retrieval and analysis. A major challenge of automated text analysis is the identification of acronyms. Acronyms, such as HTML and GAN, are abbreviations formed by combining the first letters of a series of words. Although commonly used, acronyms can often be ambiguous in meaning, with each acronym resulting in numerous possible definitions. According to Liu et al. [1], almost 81% of acronyms used in MEDLINE abstracts are ambiguous, and we expect that this value is higher when considering all webpages on the Internet. Thus, an automatic tool for identifying appropriate definitions for acronyms is essential. To address this problem, we are designing a machine-learning based classifier to match ambiguous acronyms with accurate definitions based on context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append('postgres-database/')\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import operator\n",
    "import random \n",
    "from dbFunctions import AcronymDatabase\n",
    "from sklearn.feature_extraction import text, DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree, metrics, svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "The first step in our project is preprocessing input data. The algorithm accepts HTML files as input. The urls must be loaded from an external file, and the raw text must be extracted and cleaned. There are two sets of input urls that are loaded into the model:\n",
    "- data.csv: This csv file contains a list of 1000 urls to Wikipedia articles in the categories of Medicine, Health, Biology, Computing, Electronics, and Engineering. The urls are assigned to training and testing sets in a 70:30 ratio. These Wikipedia articles come from a broad range of topics, which we are using to train/test the breadth of our model; our model needs to be able to identify definitions of a wide variety of acronyms. \n",
    "- duplicatedata.csv: This csv file contains a list of 236 urls to articles from various websites. The urls are assigned to training and testing sets in a 75:25 ratio. This dataset was manually curated to contain websites that include acronyms with multiple potential definitions. We are using these articles to train/test the depth our model; our model needs to be able to accurately identify the correct definition when presented with an ambiguous acronym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Dataset:  865\n",
      "Size of Testing Dataset:  355\n"
     ]
    }
   ],
   "source": [
    "#Load in csv data (contains list of HTML urls)\n",
    "def loadHTMLData():\n",
    "    urls = []\n",
    "    with open('data/data.csv', 'rU') as data:\n",
    "        reader = csv.reader(data, dialect=csv.excel_tab)\n",
    "        for row in reader:\n",
    "            if(len((row[0].split(','))[1]) > 0): urls.append((row[0].split(','))[1])\n",
    "    return urls\n",
    "\n",
    "def loadDuplicateData():\n",
    "    train = []\n",
    "    test = []\n",
    "    with open('data/duplicatedata.csv', 'rU') as data:\n",
    "        reader = csv.reader(data, dialect=csv.excel_tab)\n",
    "        count=0\n",
    "        for row in reader:\n",
    "            if(len((row[0].split(','))[1]) > 0): train.append((row[0].split(','))[2])\n",
    "            if(count%2 == 0 and len((row[0].split(','))[1]) > 0): train.append((row[0].split(','))[3])\n",
    "            elif(count%2 == 1 and len((row[0].split(','))[1]) > 0): test.append((row[0].split(','))[3])\n",
    "            count+=1\n",
    "    return (train, test)\n",
    "\n",
    "urls = loadHTMLData()\n",
    "trainingUrlsDuplicates = loadDuplicateData()[0] \n",
    "testingUrlsDuplicates = loadDuplicateData()[1]\n",
    "trainingUrls = trainingUrlsDuplicates + urls[:int(0.7*len(urls))]\n",
    "testingUrls = testingUrlsDuplicates + urls[int(0.7*len(urls)):]\n",
    "print ('Size of Training Dataset: ', len(trainingUrls))\n",
    "print ('Size of Testing Dataset: ', len(testingUrls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from NLTK package. Removes HTML markup from given string. \n",
    "def clean_html(html):\n",
    "    # First we remove inline JavaScript/CSS:\n",
    "    cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", html.strip().decode('ISO-8859-1'))\n",
    "    # Then we remove html comments. This has to be done before removing regular\n",
    "    # tags since comments can contain '>' characters.\n",
    "    cleaned = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned)\n",
    "    # Next we can remove the remaining tags:\n",
    "    cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
    "    # Finally, we deal with whitespace\n",
    "    cleaned = re.sub(r\"&nbsp;\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    return (cleaned.strip()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes url as input. Returns list of all acronyms in webpage\n",
    "def identifyAcronyms(rawText):\n",
    "    acronyms = []\n",
    "    #words commonly misidentified as acronyms are manually blacklisted\n",
    "    blacklist = ['ABSTRACT', 'INTRODUCTION', 'CONCLUSION', 'CONCLUSIONS', 'ACKNOWLEDGEMENTS', 'RESULTS']\n",
    "    for i in range(1,len(rawText)-1):\n",
    "        word = rawText[i]\n",
    "        word = re.sub(r'[^\\w\\s]','',word)\n",
    "        '''\n",
    "        characteristics of an acronym: all capital letters, length > 2,\n",
    "        contains only alphabet characters, not in blacklist, and not part\n",
    "        of a header (identified by determining if surrounding words are in all-caps)\n",
    "        '''\n",
    "        nextIndex = i+1\n",
    "        prevIndex = i-1\n",
    "        if(len(word)>2 and word[:-1].isupper() and word.isalpha() \\\n",
    "           and word not in blacklist and not(rawText[i-1].isupper()) \\\n",
    "           and not(rawText[i+1].isupper())):\n",
    "            acronyms.append((word, i))    \n",
    "    return acronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PostGres Database\n",
    "We have implemented a PostGres backend for this project. The database is populated with acronyms, corresponding context, and true definitions. The use of the database allows our project to be easily scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added true definitions to database\n"
     ]
    }
   ],
   "source": [
    "db = AcronymDatabase()\n",
    "#The csv file definitions.csv contains true definition labels for acronyms in each url. These labels were generated\n",
    "#programmatically with an algorithm that we designed (generatePredictedDefinitions.py), and then manually refined. \n",
    "with open('C:\\\\Users\\\\Riyansika\\\\Desktop\\\\NLP-tf\\\\AcronymLookup-master\\\\label-definitions\\\\definitions_new.csv', 'r',encoding='ISO-8859-1') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    count=0\n",
    "    for row in reader:\n",
    "        if(count<5215):\n",
    "            ret = db.addTrueDefinition(row[0], row[1].lower(), row[2])\n",
    "            count+=1\n",
    "print ('Successfully added true definitions to database')\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added all data to database\n"
     ]
    }
   ],
   "source": [
    "db = AcronymDatabase()\n",
    "for fl in (trainingUrls):\n",
    "    try:\n",
    "        html = urlopen(fl).read()\n",
    "    except:\n",
    "        continue\n",
    "    rawText = clean_html(html)\n",
    "    footerIndices = [i for i, x in enumerate(rawText) if x.lower()=='references']\n",
    "    headerIndices = [i for i, x in enumerate(rawText) if x.lower()=='abstract']\n",
    "    if(len(footerIndices)>0): rawText = rawText[:max(footerIndices)] #remove extraneous information\n",
    "    if(len(headerIndices)>0): rawText = rawText[max(headerIndices):] #remove extraneous information\n",
    "\n",
    "    def findContext(acronym, i):\n",
    "        startIndex=i-15\n",
    "        if (i-10 < 0): startIndex=0\n",
    "        endIndex = i+15 \n",
    "        if (i+10 > len(rawText)): endIndex = len(rawText)-1\n",
    "        context = []\n",
    "        for word in rawText[startIndex:endIndex+1]:\n",
    "            word = word.lower()\n",
    "            word = \"\".join(re.findall(\"[a-zA-Z]+\", word))\n",
    "            if(len(word)==0 or word==acronym.lower()): continue\n",
    "            context.append(word)\n",
    "        return \" \".join(context)\n",
    "\n",
    "    #Populate PostGres Database\n",
    "    acronyms = identifyAcronyms(rawText) #list of all acronyms and corresponding index in rawtext\n",
    "    for acronym, i in acronyms:\n",
    "        if(db.getTrueDefinition(acronym, fl)==None): continue #If definition has not been labeled, skip\n",
    "        aid = db.getAcronym(acronym) \n",
    "        if(aid==None):\n",
    "            aid = db.addAcronym(acronym)\n",
    "        true_definition = db.getTrueDefinition(acronym, fl)\n",
    "        context = findContext(acronym, i)\n",
    "        did = db.addDefinition(true_definition, context, fl, aid)\n",
    "print ('Successfully added all data to database')\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Train and Test Data\n",
    "The following section presents an analysis of the input data in our training and testing sets. We are using two testing datasets, which are differentiated as follows:\n",
    "- Test dataset 1 - Breadth: The model classifies all the acronyms in our testing set.\n",
    "- Test dataset 2 - Depth: The model classifies only the acronyms that have multiple potential definitions. Note: The depth dataset is a subset of the breadth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Training Dataset: Analysis---\n",
      "Number of unique acronyms:  2933\n",
      "Total number of occurrences of acronyms in text:  10926\n",
      "Average number of occurrences of each acronym in text:  3.7251960450051143\n",
      "Average number of definitions per acronym:  1.1186498465734742\n",
      "Standard deviation of number of definitions per acronym:  0.39200580189437756\n"
     ]
    }
   ],
   "source": [
    "#Analysis of training set:\n",
    "db = AcronymDatabase()\n",
    "uniqueAcronyms = set()\n",
    "totalOccurrences = 0\n",
    "occurrencesPerAcronym = {}\n",
    "definitionsPerAcronym = defaultdict(set)\n",
    "\n",
    "for fl in (trainingUrls):\n",
    "    try:\n",
    "        html = urlopen(fl).read()\n",
    "    except:\n",
    "        continue\n",
    "    rawText = clean_html(html)\n",
    "    footerIndices = [i for i, x in enumerate(rawText) if x.lower()=='references']\n",
    "    headerIndices = [i for i, x in enumerate(rawText) if x.lower()=='abstract']\n",
    "    if(len(footerIndices)>0): rawText = rawText[:max(footerIndices)] #remove extraneous information\n",
    "    if(len(headerIndices)>0): rawText = rawText[max(headerIndices):] #remove extraneous information\n",
    "    acronyms = identifyAcronyms(rawText) #list of all acronyms and corresponding index in rawtext\n",
    "    for acronym, i in acronyms:\n",
    "        if(db.getTrueDefinition(acronym, fl)==None): continue #If definition has not been labeled, skip\n",
    "        else:\n",
    "            uniqueAcronyms.add(acronym)\n",
    "            totalOccurrences+=1\n",
    "            definitionsPerAcronym[acronym].add(db.getTrueDefinition(acronym, fl))\n",
    "            \n",
    "            \n",
    "print ('---Training Dataset: Analysis---')\n",
    "print ('Number of unique acronyms: ', (len(uniqueAcronyms)))\n",
    "print ('Total number of occurrences of acronyms in text: ', totalOccurrences)\n",
    "print ('Average number of occurrences of each acronym in text: ', (float(totalOccurrences)/len(uniqueAcronyms)))\n",
    "numDefs=0\n",
    "for elem in definitionsPerAcronym:\n",
    "    numDefs+=len(definitionsPerAcronym[elem])\n",
    "print ('Average number of definitions per acronym: ', (float(numDefs)/len(uniqueAcronyms)))\n",
    "stdev = 0\n",
    "for elem in definitionsPerAcronym:\n",
    "    stdev += (((float(numDefs)/len(uniqueAcronyms)) - len(definitionsPerAcronym[elem]))**2)\n",
    "stdev = stdev/len(definitionsPerAcronym)\n",
    "stdev = stdev**(0.5)\n",
    "print ('Standard deviation of number of definitions per acronym: ', stdev)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Testing Dataset - Breadth: Analysis---\n",
      "Number of unique acronyms:  563\n",
      "Total number of occurrences of acronyms in text:  2318\n",
      "Average number of occurrences of each acronym in text:  4.1172291296625225\n",
      "Average number of definitions per acronym:  1.0550621669626998\n",
      "Standard deviation of number of definitions per acronym:  0.24317713804795196\n"
     ]
    }
   ],
   "source": [
    "#Analysis of testing set (breadth):\n",
    "db = AcronymDatabase()\n",
    "uniqueAcronyms = set()\n",
    "totalOccurrences = 0\n",
    "occurrencesPerAcronym = {}\n",
    "definitionsPerAcronym = defaultdict(set)\n",
    "\n",
    "for fl in (testingUrls):\n",
    "    try:\n",
    "        html = urlopen(fl).read()\n",
    "    except:\n",
    "        continue\n",
    "    rawText = clean_html(html)\n",
    "    footerIndices = [i for i, x in enumerate(rawText) if x.lower()=='references']\n",
    "    headerIndices = [i for i, x in enumerate(rawText) if x.lower()=='abstract']\n",
    "    if(len(footerIndices)>0): rawText = rawText[:max(footerIndices)] #remove extraneous information\n",
    "    if(len(headerIndices)>0): rawText = rawText[max(headerIndices):] #remove extraneous information\n",
    "    acronyms = identifyAcronyms(rawText) #list of all acronyms and corresponding index in rawtext\n",
    "    for acronym, i in acronyms:\n",
    "        if(db.getTrueDefinition(acronym, fl)==None): continue #If definition has not been labeled, skip\n",
    "        else:\n",
    "            uniqueAcronyms.add(acronym)\n",
    "            totalOccurrences+=1\n",
    "            definitionsPerAcronym[acronym].add(db.getTrueDefinition(acronym, fl))\n",
    "            \n",
    "            \n",
    "print ('---Testing Dataset - Breadth: Analysis---')\n",
    "print ('Number of unique acronyms: ', (len(uniqueAcronyms)))\n",
    "print ('Total number of occurrences of acronyms in text: ', totalOccurrences)\n",
    "print ('Average number of occurrences of each acronym in text: ', (float(totalOccurrences)/len(uniqueAcronyms)))\n",
    "numDefs=0\n",
    "for elem in definitionsPerAcronym:\n",
    "    numDefs+=len(definitionsPerAcronym[elem])\n",
    "print ('Average number of definitions per acronym: ', (float(numDefs)/len(uniqueAcronyms)))\n",
    "stdev = 0\n",
    "for elem in definitionsPerAcronym:\n",
    "    stdev += (((float(numDefs)/len(uniqueAcronyms)) - len(definitionsPerAcronym[elem]))**2)\n",
    "stdev = stdev/len(definitionsPerAcronym)\n",
    "stdev = stdev**(0.5)\n",
    "print ('Standard deviation of number of definitions per acronym: ', stdev)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.medicinenet.com/ace_inhibitors/article.htm\n",
      "https://www.medicinenet.com/script/main/art.asp?articlekey=7000\n",
      "https://blog.buildo.io/a-tour-of-abstract-syntax-trees-906c0574a067\n",
      "https://www.zetta.net/faq-what-bare-metal-recovery\n",
      "https://redmondmag.com/articles/2010/09/01/beware-the-bsa.aspx\n",
      "http://yourlegalrights.on.ca/organization/73997\n",
      "https://www.webmd.com/a-to-z-guides/complete-blood-count#1\n",
      "https://smartchurchmanagement.com/critical-success-factor-examples/\n",
      "http://www.sinobiological.com/Colony-Stimulating-Factor-CSF-Function-a-6676.html\n",
      "https://www.rxlist.com/dtp-side-effects-drug-center.htm\n",
      "https://www.mayoclinic.org/tests-procedures/eeg/basics/definition/prc-20014093\n",
      "http://www.sciencedirect.com/topics/medicine-and-dentistry/human-leukocyte-antigen\n",
      "http://anatomyandphysiologyi.com/body-fluids/\n",
      "http://www.surgeryencyclopedia.com/Fi-La/Intensive-Care-Unit.html\n",
      "http://documentation.netgear.com/reference/nld/wireless/WirelessNetworkingBasics-3-14.html\n",
      "https://www.fda.gov/AboutFDA/Transparency/Basics/ucm194951.htm\n",
      "https://www.kth.se/student/kurser/kurs/DD3344?l=en\n",
      "https://www.webmd.com/cancer/positron-emission-tomography\n",
      "https://www.tccrocks.com/blog/what-is-push-to-talk/\n",
      "https://emedicine.medscape.com/article/332244-overview\n",
      "http://www.theaviationzone.com/factsheets/sts.asp\n",
      "https://www.labcorp.com/test-menu/30706/lyme-disease-syphilis-antibodies-differential-profile\n",
      "https://emedicine.medscape.com/article/302460-overview\n",
      "---Testing Dataset - Depth: Analysis---\n",
      "Number of unique acronyms:  22\n",
      "Total number of occurrences of acronyms in text:  458\n",
      "Average number of occurrences of each acronym in text:  20.818181818181817\n",
      "Average number of definitions per acronym:  1.3636363636363635\n",
      "Standard deviation of number of definitions per acronym:  0.48104569292083466\n"
     ]
    }
   ],
   "source": [
    "#Analysis of testing set (depth):\n",
    "db = AcronymDatabase()\n",
    "uniqueAcronyms = set()\n",
    "totalOccurrences = 0\n",
    "occurrencesPerAcronym = {}\n",
    "definitionsPerAcronym = defaultdict(set)\n",
    "\n",
    "for fl in (testingUrlsDuplicates):\n",
    "    try:\n",
    "        html = urlopen(fl).read()\n",
    "    except:\n",
    "        print (fl)\n",
    "        continue\n",
    "    rawText = clean_html(html)\n",
    "    footerIndices = [i for i, x in enumerate(rawText) if x.lower()=='references']\n",
    "    headerIndices = [i for i, x in enumerate(rawText) if x.lower()=='abstract']\n",
    "    if(len(footerIndices)>0): rawText = rawText[:max(footerIndices)] #remove extraneous information\n",
    "    if(len(headerIndices)>0): rawText = rawText[max(headerIndices):] #remove extraneous information\n",
    "    acronyms = identifyAcronyms(rawText) #list of all acronyms and corresponding index in rawtext\n",
    "    for acronym, i in acronyms:\n",
    "        if(db.getTrueDefinition(acronym, fl)==None): continue #If definition has not been labeled, skip\n",
    "        else:\n",
    "            uniqueAcronyms.add(acronym)\n",
    "            totalOccurrences+=1\n",
    "            definitionsPerAcronym[acronym].add(db.getTrueDefinition(acronym, fl))\n",
    "            \n",
    "            \n",
    "print ('---Testing Dataset - Depth: Analysis---')\n",
    "print ('Number of unique acronyms: ', (len(uniqueAcronyms)))\n",
    "print ('Total number of occurrences of acronyms in text: ', totalOccurrences)\n",
    "print ('Average number of occurrences of each acronym in text: ', (float(totalOccurrences)/len(uniqueAcronyms)))\n",
    "numDefs=0\n",
    "for elem in definitionsPerAcronym:\n",
    "    numDefs+=len(definitionsPerAcronym[elem])\n",
    "print ('Average number of definitions per acronym: ', (float(numDefs)/len(uniqueAcronyms)))\n",
    "stdev = 0\n",
    "for elem in definitionsPerAcronym:\n",
    "    stdev += (((float(numDefs)/len(uniqueAcronyms)) - len(definitionsPerAcronym[elem]))**2)\n",
    "stdev = stdev/len(definitionsPerAcronym)\n",
    "stdev = stdev**(0.5)\n",
    "print ('Standard deviation of number of definitions per acronym: ', stdev)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features\n",
    "Context is used to create feature vectors for each acronym. The words surrounding an acronym are counted and stored as a feature vector. Stop words (words that provide no meaning, such as \"the\" and \"an\") are excuded from feature vectors. The acronym itself is added in as a heavily weighted feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = AcronymDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Convert training data to sparse vectors\n",
    "tokenize = CountVectorizer().build_tokenizer()\n",
    "true_defs = []\n",
    "def features(cad):\n",
    "    acronym = cad[0]\n",
    "    context = cad[1]\n",
    "    if(len(cad)==3): true_defs.append(cad[2])\n",
    "    terms = tokenize(context)\n",
    "    d = {acronym: 10}\n",
    "    for t in terms:\n",
    "        if(t not in text.ENGLISH_STOP_WORDS):\n",
    "            d[t] = d.get(t, 0) + 1\n",
    "    return d\n",
    "\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "cadList = db.getContextAcronymList()\n",
    "vect = DictVectorizer()\n",
    "X_train = vect.fit_transform(features(d) for d in cadList)\n",
    "print (X_train.toarray())\n",
    "#print cadList[2]\n",
    "#print vect.get_feature_names()\n",
    "#print true_defs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Machine Learning Classifier\n",
    "A machine learning classifier was created and fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune hyperparameters\n",
    "parameter_candidates = [\n",
    "  {'C': [1], \n",
    "   'gamma': [0.001, 0.0001]}\n",
    "]\n",
    "parameters_nb = [\n",
    "    {'alpha': [0.01, 0.1, 0.5, 1]}\n",
    "]\n",
    "parameters_tree = [{\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 2, None],\n",
    "              \"min_samples_leaf\": [1, 2, 3],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1).fit(X_train, true_defs)\n",
    "print('Best C:',clf.best_estimator_.C) \n",
    "print('Best Kernel:',clf.best_estimator_.kernel)\n",
    "print('Best Gamma:',clf.best_estimator_.gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(estimator=MultinomialNB(), param_grid=parameters_nb, n_jobs=-1, cv=2).fit(X_train, true_defs)\n",
    "print('Best alpha:',clf.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(estimator=tree.DecisionTreeClassifier(), param_grid=parameters_tree, n_jobs=-1).fit(X_train, true_defs)\n",
    "print('Best Max Depth:',clf.best_estimator_.max_depth) \n",
    "print('Best Max Features:',clf.best_estimator_.max_features)\n",
    "print('Best Min Samples:',clf.best_estimator_.min_samples_leaf)\n",
    "print('Best Criterion:',clf.best_estimator_.criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = MultinomialNB(alpha=0.1).fit(X_train, true_defs)\n",
    "print ('Trained Model 1')\n",
    "clf2 = svm.LinearSVC(C=1).fit(X_train, true_defs)\n",
    "print ('Trained Model 2')\n",
    "clf3 = tree.DecisionTreeClassifier(min_samples_leaf=1).fit(X_train, true_defs)\n",
    "print ('Trained Model 3')\n",
    "clf4 = RandomForestClassifier().fit(X_train, true_defs)\n",
    "print ('Trained Model 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Machine Learning Classifier\n",
    "The accuracy of the classifier was tested on the training dataset and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate training accuracy\n",
    "trainData = []\n",
    "y_true = []\n",
    "for fl in (trainingUrls):\n",
    "    #print \"URL Index: %d\" % urls.index(fl)\n",
    "    try:\n",
    "        html = urlopen(fl).read()\n",
    "    except:\n",
    "        continue\n",
    "    rawText = clean_html(html)\n",
    "    footerIndices = [i for i, x in enumerate(rawText) if x.lower()=='references']\n",
    "    headerIndices = [i for i, x in enumerate(rawText) if x.lower()=='abstract']\n",
    "    if(len(footerIndices)>0): rawText = rawText[:max(footerIndices)] #remove extraneous information\n",
    "    if(len(headerIndices)>0): rawText = rawText[max(headerIndices):] #remove extraneous information\n",
    "\n",
    "    def findContext(acronym, i):\n",
    "        startIndex=i-15\n",
    "        if (i-10 < 0): startIndex=0\n",
    "        endIndex = i+15 \n",
    "        if (i+10 > len(rawText)): endIndex = len(rawText)-1\n",
    "        context = []\n",
    "        for word in rawText[startIndex:endIndex+1]:\n",
    "            word = word.lower()\n",
    "            word = \"\".join(re.findall(\"[a-zA-Z]+\", word))\n",
    "            if(len(word)==0 or word==acronym.lower()): continue\n",
    "            context.append(word)\n",
    "        return \" \".join(context)\n",
    "\n",
    "    #Populate PostGres Database\n",
    "    acronyms = identifyAcronyms(rawText) #list of all acronyms and corresponding index in rawtext\n",
    "    for acronym, i in acronyms:\n",
    "        if(db.getTrueDefinition(acronym, fl)==None): continue #Definition has been labeled in database\n",
    "        context = findContext(acronym, i)\n",
    "        trainData.append((acronym, context))\n",
    "        y_true.append(db.getTrueDefinition(acronym, fl))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_counts = vect.transform(features(d) for d in trainData)\n",
    "predicted1 = clf1.predict(X_new_counts)\n",
    "predicted2 = clf2.predict(X_new_counts)\n",
    "predicted3 = clf3.predict(X_new_counts)\n",
    "predicted4 = clf4.predict(X_new_counts)\n",
    "#for train, definition, true in zip(trainData, predicted, y_true):\n",
    "#    if(true!=definition): print('%s => %s, %s' % (train[0], definition, true))\n",
    "\n",
    "#print metrics.precision_recall_fscore_support(y_true, predicted, average='weighted')\n",
    "print (\"Prediction Accuracy - Multinomial NB: \", accuracy_score(y_true, predicted1))\n",
    "print (metrics.precision_recall_fscore_support(y_true, predicted1, average='weighted'))\n",
    "print (\"Prediction Accuracy - SVC: \", accuracy_score(y_true, predicted2))\n",
    "print (metrics.precision_recall_fscore_support(y_true, predicted2, average='weighted'))\n",
    "print (\"Prediction Accuracy - Decision Tree: \", accuracy_score(y_true, predicted3))\n",
    "print (metrics.precision_recall_fscore_support(y_true, predicted3, average='weighted'))\n",
    "print (\"Prediction Accuracy - Random Forest: \", accuracy_score(y_true, predicted4))\n",
    "print (metrics.precision_recall_fscore_support(y_true, predicted4, average='weighted'))\n",
    "#print(metrics.classification_report(y_true, predicted1))\n",
    "#print(metrics.classification_report(y_true, predicted2))\n",
    "#print(metrics.classification_report(y_true, predicted3))\n",
    "#print(metrics.classification_report(y_true, predicted4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = []\n",
    "y_true_test = []\n",
    "for fl in (testingUrls):\n",
    "    #print \"URL Index: %d\" % urls.index(fl)\n",
    "    try:\n",
    "        html = urlopen(fl).read()\n",
    "    except:\n",
    "        continue\n",
    "    rawText = clean_html(html)\n",
    "    footerIndices = [i for i, x in enumerate(rawText) if x.lower()=='references']\n",
    "    headerIndices = [i for i, x in enumerate(rawText) if x.lower()=='abstract']\n",
    "    if(len(footerIndices)>0): rawText = rawText[:max(footerIndices)] #remove extraneous information\n",
    "    if(len(headerIndices)>0): rawText = rawText[max(headerIndices):] #remove extraneous information\n",
    "\n",
    "    def findContext(acronym, i):\n",
    "        startIndex=i-15\n",
    "        if (i-10 < 0): startIndex=0\n",
    "        endIndex = i+15 \n",
    "        if (i+10 > len(rawText)): endIndex = len(rawText)-1\n",
    "        context = []\n",
    "        for word in rawText[startIndex:endIndex+1]:\n",
    "            word = word.lower()\n",
    "            word = \"\".join(re.findall(\"[a-zA-Z]+\", word))\n",
    "            if(len(word)==0 or word==acronym.lower()): continue\n",
    "            context.append(word)\n",
    "        return \" \".join(context)\n",
    "\n",
    "    #Populate PostGres Database\n",
    "    acronyms = identifyAcronyms(rawText) #list of all acronyms and corresponding index in rawtext\n",
    "    for acronym, i in acronyms:\n",
    "        if(db.getTrueDefinition(acronym, fl)==None): continue #Definition has been labeled in database\n",
    "        if(db.getTrueDefinition(acronym, fl) not in y_true): continue\n",
    "        context = findContext(acronym, i)\n",
    "        testData.append((acronym, context))\n",
    "        y_true_test.append(db.getTrueDefinition(acronym, fl))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_counts = vect.transform(features(d) for d in testData)\n",
    "predicted1 = clf1.predict(X_new_counts)\n",
    "predicted2 = clf2.predict(X_new_counts)\n",
    "predicted3 = clf3.predict(X_new_counts)\n",
    "predicted4 = clf4.predict(X_new_counts)\n",
    "#for test, definition, true in zip(testData, predicted, y_true_test):\n",
    "    #if(definition==true): print('Correct: %s => %s, %s' % (test[0], definition, true))\n",
    "#    if(definition!=true): print('Error: %s => %s, %s' % (test[0], definition, true))\n",
    "print (\"Prediction Accuracy - Multinomial NB: \", accuracy_score(y_true_test, predicted1))\n",
    "print (metrics.precision_recall_fscore_support(y_true_test, predicted1, average='weighted'))\n",
    "print (\"Prediction Accuracy - SVC: \", accuracy_score(y_true_test, predicted2))\n",
    "print (metrics.precision_recall_fscore_support(y_true_test, predicted2, average='weighted'))\n",
    "print (\"Prediction Accuracy - Decision Tree: \", accuracy_score(y_true_test, predicted3))\n",
    "print (metrics.precision_recall_fscore_support(y_true_test, predicted3, average='weighted'))\n",
    "print (\"Prediction Accuracy - Random Forest: \", accuracy_score(y_true_test, predicted4))\n",
    "print (metrics.precision_recall_fscore_support(y_true_test, predicted4, average='weighted'))\n",
    "#print(metrics.classification_report(y_true_test, predicted1))\n",
    "#print(metrics.classification_report(y_true_test, predicted2))\n",
    "#print(metrics.classification_report(y_true_test, predicted3))\n",
    "#print(metrics.classification_report(y_true_test, predicted4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.0f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    #for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        #plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 #horizontalalignment=\"center\",\n",
    "                 #color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix1 = confusion_matrix(y_true_test, predicted1)\n",
    "cnf_matrix2 = confusion_matrix(y_true_test, predicted2)\n",
    "cnf_matrix3 = confusion_matrix(y_true_test, predicted3)\n",
    "cnf_matrix4 = confusion_matrix(y_true_test, predicted4)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "#plt.figure()\n",
    "class_names1 = sorted(list(set(predicted1).union(set(y_true_test))))\n",
    "class_names2 = sorted(list(set(predicted2).union(set(y_true_test))))\n",
    "class_names3 = sorted(list(set(predicted3).union(set(y_true_test))))\n",
    "class_names4 = sorted(list(set(predicted4).union(set(y_true_test))))\n",
    "#plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "#                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(cnf_matrix1, classes=class_names1, normalize=True,\n",
    "                      title='Normalized confusion matrix - Multinomial NB')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(cnf_matrix2, classes=class_names2, normalize=True,\n",
    "                      title='Normalized confusion matrix - SVC')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(cnf_matrix3, classes=class_names3, normalize=True,\n",
    "                      title='Normalized confusion matrix - Decision Tree')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(cnf_matrix4, classes=class_names4, normalize=True,\n",
    "                      title='Normalized confusion matrix - Random Forest')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataDuplicates = []\n",
    "y_true_test_duplicates = []\n",
    "for fl in (testingUrlsDuplicates):\n",
    "    try:\n",
    "        html = urlopen(fl).read()\n",
    "    except:\n",
    "        continue\n",
    "    rawText = clean_html(html)\n",
    "    footerIndices = [i for i, x in enumerate(rawText) if x.lower()=='references']\n",
    "    headerIndices = [i for i, x in enumerate(rawText) if x.lower()=='abstract']\n",
    "    if(len(footerIndices)>0): rawText = rawText[:max(footerIndices)] #remove extraneous information\n",
    "    if(len(headerIndices)>0): rawText = rawText[max(headerIndices):] #remove extraneous information\n",
    "\n",
    "    def findContext(acronym, i):\n",
    "        startIndex=i-15\n",
    "        if (i-10 < 0): startIndex=0\n",
    "        endIndex = i+15 \n",
    "        if (i+10 > len(rawText)): endIndex = len(rawText)-1\n",
    "        context = []\n",
    "        for word in rawText[startIndex:endIndex+1]:\n",
    "            word = word.lower()\n",
    "            word = \"\".join(re.findall(\"[a-zA-Z]+\", word))\n",
    "            if(len(word)==0 or word==acronym.lower()): continue\n",
    "            context.append(word)\n",
    "        return \" \".join(context)\n",
    "\n",
    "    #Populate PostGres Database\n",
    "    acronyms = identifyAcronyms(rawText) #list of all acronyms and corresponding index in rawtext\n",
    "    for acronym, i in acronyms:\n",
    "        if(db.getTrueDefinition(acronym, fl)==None): continue #Definition has been labeled in database\n",
    "        if(db.getTrueDefinition(acronym, fl) not in y_true): continue\n",
    "        context = findContext(acronym, i)\n",
    "        testDataDuplicates.append((acronym, context))\n",
    "        y_true_test_duplicates.append(db.getTrueDefinition(acronym, fl))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_counts_duplicates = vect.transform(features(d) for d in testDataDuplicates)\n",
    "predicted1 = clf1.predict(X_new_counts_duplicates)\n",
    "predicted2 = clf2.predict(X_new_counts_duplicates)\n",
    "predicted3 = clf3.predict(X_new_counts_duplicates)\n",
    "predicted4 = clf4.predict(X_new_counts_duplicates)\n",
    "\n",
    "#for test, definition, true in zip(testDataDuplicates, predicted, y_true_test_duplicates):\n",
    "    #if(definition==true): print('Correct: %s => %s, %s' % (test[0], definition, true))\n",
    "#    if(definition!=true): print('Error: %s => %s, %s' % (test[0], definition, true))\n",
    "print (\"Prediction Accuracy - Multinomial NB: \", accuracy_score(y_true_test_duplicates, predicted1))\n",
    "print (metrics.precision_recall_fscore_support(y_true_test_duplicates, predicted1, average='weighted'))\n",
    "print (\"Prediction Accuracy - SVC: \", accuracy_score(y_true_test_duplicates, predicted2))\n",
    "print (metrics.precision_recall_fscore_support(y_true_test_duplicates, predicted2, average='weighted'))\n",
    "print (\"Prediction Accuracy - Decision Tree: \", accuracy_score(y_true_test_duplicates, predicted3))\n",
    "print (metrics.precision_recall_fscore_support(y_true_test_duplicates, predicted3, average='weighted'))\n",
    "print (\"Prediction Accuracy - Random Forest: \", accuracy_score(y_true_test_duplicates, predicted4))\n",
    "print (metrics.precision_recall_fscore_support(y_true_test_duplicates, predicted4, average='weighted'))\n",
    "#print(metrics.classification_report(y_true_test_duplicates, predicted1))\n",
    "#print(metrics.classification_report(y_true_test_duplicates, predicted2))\n",
    "#print(metrics.classification_report(y_true_test_duplicates, predicted3))\n",
    "#print(metrics.classification_report(y_true_test_duplicates, predicted4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix1 = confusion_matrix(y_true_test_duplicates, predicted1)\n",
    "cnf_matrix2 = confusion_matrix(y_true_test_duplicates, predicted2)\n",
    "cnf_matrix3 = confusion_matrix(y_true_test_duplicates, predicted3)\n",
    "cnf_matrix4 = confusion_matrix(y_true_test_duplicates, predicted4)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "#plt.figure(figsize=(30,30))\n",
    "class_names1 = sorted(list(set(predicted1).union(set(y_true_test_duplicates))))\n",
    "class_names2 = sorted(list(set(predicted2).union(set(y_true_test_duplicates))))\n",
    "class_names3 = sorted(list(set(predicted3).union(set(y_true_test_duplicates))))\n",
    "class_names4 = sorted(list(set(predicted4).union(set(y_true_test_duplicates))))\n",
    "#print len(class_names)\n",
    "#plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "#                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(cnf_matrix1, classes=class_names1, normalize=True,\n",
    "                      title='Normalized confusion matrix - Multinomial NB')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(cnf_matrix2, classes=class_names2, normalize=True,\n",
    "                      title='Normalized confusion matrix - SVC')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(cnf_matrix3, classes=class_names3, normalize=True,\n",
    "                      title='Normalized confusion matrix - Decision Tree')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(cnf_matrix4, classes=class_names4, normalize=True,\n",
    "                      title='Normalized confusion matrix - Random Forest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
